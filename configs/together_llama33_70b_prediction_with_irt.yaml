name: "llama-3.3-70b-prediction-irt" 
description: "Together AI Llama 3.3 70B for switch prediction with IRT timing data"
provider: "together"
model: "meta-llama/Llama-3.3-70B-Instruct-Turbo"
type: "prediction"
version: "1.0"

# Name must be "anthropic" or "together"
# Provider must be "anthropic" or "together"
# Type must be "switches", "labels", or "prediction"

# Model parameters - all optional, defaults depend on provider
parameters:
  max_tokens: 1500 # Required
  temperature: 0.1  # Low for consistency
  top_p: 0.9

# Include inter-item response times (IRT) in prompts
# When true, prompts will show timing data: "0. cat (2800ms)"
# When false or omitted, only shows words: "0. cat"
include_irt: true

# Custom prompt template (optional, overrides default)
# Available variables when include_irt is true:
  # - {category} - The verbal fluency category
  # - {chunk_words} - Formatted word list with timing ("0. cat (2800ms)\n1. dog (834ms)")
  # - {num_words} - Number of words in chunk sequence  
  # - {chunk_length} - Same as num_words
  # - {irt_context} - Description of IRT timing context
  
prompt_template: null 

# Metadata
created_date: "2025-01-19"
author: "maria"
notes: "Enhanced switch prediction using inter-item response times to improve prediction accuracy. Uses TogetherAI's JSON mode for structured output."

# Expected output format
output_format:
  type: "json"
  schema:
    prediction: "integer (0 or 1)"
    confidence: "float (0.0 to 1.0)"
    reasoning: "string explaining the prediction"